{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061d5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "\n",
    "from langchain.schema import Document \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from qdrant_client import QdrantClient as RawQdrantClient \n",
    "from qdrant_client.http import models \n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_groq import ChatGroq \n",
    "from docx import Document as DocxDocument \n",
    "\n",
    "# Suppress specific pdfplumber warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"CropBox missing from /Page, defaulting to MediaBox\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"QDRANT_API_KEY\"] = os.getenv(\"QDRANT_API_KEY\")\n",
    "os.environ[\"QDRANT_URL\"] = os.getenv(\"QDRANT_URL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed4292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "PDF_PATH = \"Principles-of-Data-Science-WEB.pdf\" # Make sure this PDF is in the same directory or provide full path\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_SIZE = 384 # Dimension of \"all-MiniLM-L6-v2\"\n",
    "GROQ_MODEL_NAME = \"gemma2-9b-it\"\n",
    "\n",
    "CONTENT_KEY_IN_PAYLOAD = \"text_content_for_langchain\"\n",
    "\n",
    "# Define Qdrant collection naming convention\n",
    "COLLECTION_NAME_PREFIX = \"data_science_demo\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e57999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- PDF Data Extraction Function ---\n",
    "def extract_pdf_with_sources(pdf_path):\n",
    "    \"\"\"Extracts text and table content from a PDF, retaining source and page info.\"\"\"\n",
    "    documents = []\n",
    "    print(f\"Extracting text content from '{os.path.basename(pdf_path)}'...\")\n",
    "    # Extract text pages with PyMuPDF\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                text = page.get_text()\n",
    "                if text.strip():\n",
    "                    metadata = {\n",
    "                        \"source\": os.path.basename(pdf_path),\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"type\": \"text\"\n",
    "                    }\n",
    "                    documents.append(Document(page_content=text, metadata=metadata))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        # Do not exit, attempt table extraction\n",
    "        pass\n",
    "\n",
    "    print(f\"Extracting table content from '{os.path.basename(pdf_path)}'...\")\n",
    "    # Extract tables with pdfplumber\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()\n",
    "                for table_num, table_data in enumerate(tables):\n",
    "                    if table_data:\n",
    "                        table_content = \"\\n\".join([\"\\t\".join(map(str, row)) for row in table_data if row])\n",
    "                        if table_content.strip():\n",
    "                            metadata = {\n",
    "                                \"source\": os.path.basename(pdf_path),\n",
    "                                \"page\": page_num + 1,\n",
    "                                \"table_num\": table_num + 1,\n",
    "                                \"type\": \"table\"\n",
    "                            }\n",
    "                            # Add a header to table content to distinguish it\n",
    "                            documents.append(Document(page_content=f\"Table {table_num+1} on page {page_num+1}:\\n{table_content}\", metadata=metadata))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from {pdf_path}: {e}\")\n",
    "        # Continue even if table extraction fails\n",
    "        pass\n",
    "\n",
    "    documents = [doc for doc in documents if doc.page_content.strip()] # Final filter for empty content\n",
    "    print(f\"Extracted {len(documents)} raw documents (pages/tables) from '{os.path.basename(pdf_path)}'.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d38a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Document Formatting Function for LLM Context ---\n",
    "def format_docs_for_context(docs):\n",
    "    \"\"\"\n",
    "    Formats a list of retrieved Documents into a single string suitable for LLM context.\n",
    "    Includes source information.\n",
    "    \"\"\"\n",
    "    formatted_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        source_info_parts = []\n",
    "        source_info_parts.append(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "        source_info_parts.append(f\"Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "        if doc.metadata.get('type') == 'table' and doc.metadata.get('table_num') is not None:\n",
    "             source_info_parts.append(f\"Table: {doc.metadata.get('table_num')}\")\n",
    "        elif doc.metadata.get('type') == 'text' and doc.metadata.get('start_index') is not None:\n",
    "             # Optionally include chunk start index, though page is usually sufficient for LLMs\n",
    "             # source_info_parts.append(f\"Chunk Start: {doc.metadata.get('start_index')}\")\n",
    "             pass # Don't include start_index in typical source info for LLM\n",
    "\n",
    "        source_info = \", \".join(source_info_parts)\n",
    "\n",
    "        # Adjust the formatting here for how context appears to the LLM\n",
    "        formatted_text += f\"--- Document {i+1} ({source_info}) ---\\n\"\n",
    "        formatted_text += doc.page_content.strip() + \"\\n\\n\"\n",
    "    return formatted_text.strip() # Remove trailing newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_manually(query: str, collection_name: str, embeddings_model, k: int = 3):\n",
    "    \"\"\"\n",
    "    Performs a vector search using the raw Qdrant client and manually constructs\n",
    "    LangChain Document objects with correct metadata from the payload.\n",
    "\n",
    "    Args:\n",
    "        query: The user's search query.\n",
    "        collection_name: The name of the Qdrant collection to search.\n",
    "        embeddings_model: The embedding model instance used for the query.\n",
    "        k: The number of top results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of LangChain Document objects with correctly populated page_content and metadata.\n",
    "    \"\"\"\n",
    "    # print(f\"\\nPerforming manual retrieval for collection '{collection_name}'...\")\n",
    "    try:\n",
    "        # 1. Embed the query\n",
    "        query_vector = embeddings_model.embed_query(query)\n",
    "\n",
    "        # 2. Perform the vector search using the raw Qdrant client\n",
    "        # Ensure qdrant_api_client is accessible in this scope (it's defined globally below)\n",
    "        search_result_points = qdrant_api_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=k,\n",
    "            with_payload=True, # Ensure payload is returned with results - CRITICAL\n",
    "            with_vectors=False # No need to fetch vectors\n",
    "        )\n",
    "\n",
    "        # 3. Manually construct LangChain Document objects from the results\n",
    "        manually_created_docs = []\n",
    "        if search_result_points:\n",
    "            # print(f\"Raw Qdrant search returned {len(search_result_points)} points. Constructing Documents...\")\n",
    "            for i, point in enumerate(search_result_points):\n",
    "                # Verify payload and content key exist\n",
    "                if point.payload and CONTENT_KEY_IN_PAYLOAD in point.payload:\n",
    "                    doc_content = point.payload[CONTENT_KEY_IN_PAYLOAD]\n",
    "\n",
    "                    # Copy all other payload keys into the metadata dictionary\n",
    "                    doc_metadata = {k: v for k, v in point.payload.items() if k != CONTENT_KEY_IN_PAYLOAD}\n",
    "\n",
    "                    # Optionally, add the search score to the metadata\n",
    "                    doc_metadata[\"score\"] = point.score\n",
    "\n",
    "                    manually_created_docs.append(\n",
    "                        Document(page_content=doc_content, metadata=doc_metadata)\n",
    "                    )\n",
    "                else:\n",
    "                     # This warning is helpful during debugging, maybe less so in a final run\n",
    "                     # print(f\"  Warning: Point ID {point.id} from search result did not have expected payload or content key.\")\n",
    "                     pass # Skip points with missing content key\n",
    "\n",
    "        # print(f\"Successfully constructed {len(manually_created_docs)} LangChain Documents.\")\n",
    "        return manually_created_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during manual search or document construction for '{collection_name}': {e}\")\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n",
    "        return [] # Return empty list in case of error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AgenticAI)",
   "language": "python",
   "name": "agenticai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
